
Okay enterprise Data Platform (EDP) this is a platform built on Snowflake in the cloud. The cloud platform is AWS, and our tool for transformation is DBT. We extract data from our on-prem systems and use a CDC method. This is typically a SQL Server, and that CDC stream goes to an S3 bucket in AWS as a CSV file. The CSV file is then ingested into Snowflake using a SnowPipe and a stream. Some of that stream is used for near real-time use cases, but the majority of it is only loaded every 4 hours in batches. The near real-time use cases are set up with views that look at the physical historical table of all the changes coming from that source and then union it with the contents of the stream in Snowflake to get a dynamic sort of real-time-ish view. That then goes into a couple of layers of dynamic tables so that the data can be transformed and available for consumption as quickly as possible. Target for near real-time use cases, we're still working on but our current goal is 10 minutes or less of latency. The raw layer is specifically designed and organized by source systems. It is intended to be an immutable audit of the source system changes. 

We read the stream and create what we call the raw layer (in Medallion architecture would be the Bronze layer).We have one set of schemas in the raw layer for each source system database. So if a source system was called Gemstone for instance and it had five different databases on it, the raw layer would contain both a history and a transient schema. The transient schema is for the stream, and the history schema is for the physical table.  

The data then moves to what we call the integration layer, or in the Medallion architecture, that would be called the silver layer. The integration layer is where we start organizing data by domain rather than by source. This is also where we tie the similar data from different sources together, hence the term integration. This is where we introduce source system agnostic terms. We do a little bit of light data cleansing only for sort of dangerous unprintable characters and things like that. We also apply our record identification process here. Currently, we are transitioning our integration layer to follow the raw vault methodology portion of Data Vault 2.0. So integration layer = Silver layer = raw vault, if that makes sense. 

Next layer is the curation layer, which would be the Gold layer in Medallion architecture and the Business Vault under Data Vault methodology. The Curation Layer is a fit-for-you set of data, organized by data domain or by business process for very large and unique business processes. And here is where we apply our enterprise business rules and other transformations, such as:
1. Reshaping the data into a dimensional model
2. Flattening the data set for a machine learning model or extract
3. Making your data available via API

We have one more layer. Not sure what it would be referred to as in the Medallion architecture. But we call it the consumption layer. It's essentially a partially materialized semantic layer. Under data vault methodology, it would be called the information mart. This data is intended to be fit for purpose, so it is typically selections from the curation layer that are exposed to a targeted group of people for a specific business purpose. There's also some custom logic capabilities here as well. These would fall under data transformation that is not enterprise business rules, so not something that's in common across the enterprise. Typically this is because we are preparing the data for an outside party. And have to send data on a regular basis to this third party. Examples of this would be:
- CMS audit information
- The Edge server (which is related to the QHP line of business) has a very custom process
- A national data warehouse for the Blue Cross Blue Shield Association that we participate in, which gets a monthly feed of claims, membership, and provider information
- Data science style experimenting with projections and data manipulation. 

There's another database that, with those four layers, comprises an environment. This is called the Common Database, and it is intended to hold things like logging or functional infrastructure pieces that are needed for either tooling or for tracking the results of processes. 

The four layers plus the common database are expressed as five different databases in total:
- One for each layer
- One for the common technical metadata

That set of databases is replicated for each environment that we have within our Snowflake account. At present, we have a dev environment, a test environment, and a prod environment. Outside of the environments, there are two databases that are important: there's a Snowflake admin database for database administration and platform administration. And then there's a data governance database which is limited to our data governance folks, and it holds all of the row access policies and data masking policies and the logic that goes with that. 

Once the data is in Snowflake, the tool that we use for data transformations is DBT. Since we are following the Data Vault methodology, we use a DBT package called AutomateDV, which makes it much easier to create each of the Data Vault artifact types.

In general, in our environment, data flows from higher environments down to lower environments. And code flows up from lower environments to higher environments. The way this gets expressed in our platform is that the PROD raw database for the PROD environment gets replicated to the test and dev environments. The code that we work on against the dev environment is then moved, released to test, and finally to prod through a release management process. There is one slight variation to this because of the data flowing downward, and there's sort of a chicken-and-egg problem: we need to do development for ingestion purposes before that ingestion is available in production. So that development happens in the dev environment in a separate copy of the raw database. In our dev environment, there are two raw databases:
1. dev_raw_db, which is for the ingestion team to do their development in
2. That then gets promoted to production after approval by data governance
There's also dev_raw_clone_db. This database is for our DBT development and is just a zero-copy clone of prod raw DB. Also in our dev environment, there is a special database for dbt development. It is called dev_dbt_transform_db. This database holds individual schemas for our DBT developers. By developer username, there's a schema for each one. As developers create a branch and work on features, data that they process and troubleshoot goes into their developer schema while they are working on it and moves into the dev environment at large when they release it to our develop branch. 

# AI leverage opportunities

The previous version of our integration layer was actually built in a third normal form data architecture. We are converting that third normal form architecture over to Data Vault architecture. That's an ongoing code refactoring process that I am trying to build an AI prompt to help me complete. 

Another opportunity we have for AI leverage in this project is we are migrating from our on-prem systems into the cloud and part of that migration is replicating the data pipelines that are on-prem that build our current dimensional model and other informational and analytical needs. That logic is in SQL Server stored procedures that are generated by a program called Warescape in our on-prem environments. These stored procedures build our dimensional model and a bunch of business-centric data views to help them with their other analytics and reporting outside of the dimensional model. The task we have in front of us is to port all of that capability into the cloud. We do not want to do a lift and shift, we are very concerned about the compute costs associated with our on-prem solution if we just translated them directly into Snowflake. But the on-prem system does have a sort of raw vault type of structure. So I would like to create a process to take the business rule code from the on-prem system and translate it into DBT pipelines. The on-prem system has a raw vault and then built directly on top of it are stored procedures that create a dimensional model. They're quite complicated and long series of stored procedures that contribute to each artifact in the dimensional model. We are rebuilding the raw vault in our integration layer as I described earlier. I'd like a process that remaps the stored procedures, documents the business rules involved, and remaps the raw vault from the on-prem system to the raw vault tables in the Snowflake system. And then using that mapping and the SQL stored procedure logic, we create DBT pipelines that create business vault artifacts and dimensional model artifacts to streamline our data pipelines and adhere more closely to Data Vault 2.0 methodology. Our on-prem Data Vault project did fairly well with the Raw Vault, although there are a few things that we need to change. But the business vault, we really didn't do that at all. We just went straight from Raw Vault into a dimensional model. So, two- three main tasks with that migration code refactoring:
1. Document business rules involved
2. Create business vault artifacts where we should have had them before
3. Create the dimensional model artifacts

Those two use cases are targeted use cases that will be somewhat temporary.
- The initial conversion from third normal form over to Data Vault will be just a few more months' worth of work
- The translation of the EDW code into Snowflake will be a much longer planning horizon project, but it's still temporary, you know, a year or something to replicate all of that
But there's a third use case that I think would be a more permanent one: to leverage AI during the design process to actually create starter code based on a data model. Since we are using raw vault methodology, I think we can give enough guidelines to a process that I could take a diagram of a hub-link and satellite combination of tables, an aggregate or an entity of several related physical tables that need to be delivered together. Within those small units of work, I think we can guide the creation of the code according to our platform architecture and naming conventions. I think we should be able to get pretty close to runnable code with only a little bit of input. 