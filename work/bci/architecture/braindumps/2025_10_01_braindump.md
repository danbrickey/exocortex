EDP documentation 

1. Data Ingestion & CDC Implementation
Missing Details:
CDC Tool/Mechanism: For CEC, the standard for our company is GitLab CI/CD pipelines. For DBT, we use some built-in DBT capability to set up jobs that are CI/CD jobs. Those for DBT are in the scheduler, and we're thinking about switching them to GitLab just for consistency purposes. But everything else the companies use GitLab CI/CD pipelines
Stream Consumption: You mentioned streams are consumed every 4 hours - what task scheduler orchestrates this? These would be snowflake tasks. At least for ingestion, and then from that point on, dbt takes over and uses a combination of time jobs and dynamic tables depending on the use case.
2. Near-Real-Time Architecture
Questions:
Dynamic Table Specifics: The dynamic tables for the near real-time use cases will be in at least one spot which is the curation layer where we apply business rules because we have to do significant transformation to some of the data, and so we felt that would be best to target physical tables for that. The rest of the layers from the raw data we are hoping we can make virtual (i.e., virtualize those layers). But if we have to, we can create a set of dynamic tables in the integration layer as well

Latency Monitoring: The latency question is more of "how little latency can we get for this price" as opposed to the next price point by sizing up warehouses, etc. So, the latency will be several seconds of lag, probably under a minute, but close to a minute of lag between the source and the S3 bucket and the Snow Pipe. Once it's in the stream - Snow Pipe stream - then we can see it virtually. After that, that path is about we estimate to be right around a minute. The integration layer is a view layer. That's what we planned. The curation layer is a set of dynamic tables. Maybe a couple layers depending on how much business rule processing we have to do. The consumption layer is just a set of views.

Use Case Details: Right now, the real-time use case is for driving a customer service portal that will be used by our customer service agents. It's time sensitive - they need to know real-time changes to eligibility and claims, and things like deductibles and so on. We may have other real-time use cases, so I'm trying to keep it generic. But for right now, that's the one use case we're serving in a real-time sense.

Failure Handling: Failure modes for real-time are undetermined. Hopefully, they will come out during testing when we try to prove this out and see if we can get it to happen in the amount of time we need. And also for the amount of money that we're constrained to.

3. Integration Layer / Raw Vault Details
Questions:
Schema Organization: You mentioned organizing by domain in the integration layer. What are the specific domain schemas? (e.g., member, provider, claims, financial?) 
domains:

provider
member
product
claims
broker
clinical
financial

Record Identification Algorithm: You mentioned a "record identification process" - is this documented anywhere? What specifically does it do beyond preventing key collisions? Yeah, it's really about preventing key collisions. We're using a hashing scheme in line with DataVault 2.0 methodology. It consists of the source system identifier and the primary key of the source data if we're enforcing that as a business key in a hub or a link.

Current Views Strategy: Are current views auto-generated from satellites, or manually created? Is there a pattern/template? Yeah, current views are auto-generated as part of our code generation process that I'll be working on an updated version of in this project. But we have a refactoring process that's somewhat agentic in terms of just refined par-ops and some input files and output specs. Anyway, they generate the current view based on our architectural specifications we supplied, but it's just a current view of everything in all the satellites for that hub or link.

3NF Refactoring Timeline: What's the scope/timeline for refactoring existing 3NF to Data Vault? Which entities are done vs. remaining? The 3NF refactoring is also underway. I've built a somewhat agentic process for that to input some specs and get out refactored code. That's going pretty smoothly. I'll probably be making an updated version in this project so I can try to get it to run a little bit more efficiently as the current process on-prem takes quite a while. It's pretty slow, mostly because of copying and pasting and waiting and overflowing context windows and things like that which I think I can avoid in Claude Code. The timeline is expected to be the next two or three months unless we can accelerate the process even more than we already have. I would say we are about 30% converted, refactored over to data vault, and we have about 60-70% left to go. Of the things that have currently been built in our integration layer. And then after we build those things, we have a very large list of things that need to be developed for the raw vault so that we can then develop business vaults. So what we really have is a very core set of universal dependencies in our data that have to do with eligibility and claims, but we definitely need to enrich that.

4. Master Data Management
Missing Context:
Member/Person Matching: You mentioned a "rudimentary" algorithm - what exactly does it do? What attributes does it match on?
Provider Matching: Same question - what's the algorithm logic? The On-Prem Person Identification Algorithm just looks at name, birthdate, gender, social security number in different combinations. When we have a new member record, it goes and checks all existing member records to see if there's another member that we've already issued an ID for that seems to be the same person. If it is, then we tag them with that same ID number that we used before. If not, then we create a brand new ID number and attach it to them as a new person. It doesn't really have a gold record like the best address or best name or most recent name or whatever. That's why I said it's rudimentary; it's based on very deterministic rules and it is limited in scope. In terms of future goals for MDM, we definitely want to go about the process more seriously than we have on Prem and create something more sound that uses deterministic logic at first but also has more advanced algorithms that we can run to identify members and identify households they belong to and so on.

Gold Records: You mentioned no gold MDM records exist - is this planned? What's the strategy? Gold records are planned - just something that we haven't done. We are starting up a new MDM initiative as past MDM initiatives have gone poorly, and we're kind of rebooting the whole process. We are starting with the domains of:
1. Member
2. Provider
3. Product

Confidence Scores: Do matching algorithms provide confidence scores? How are conflicts resolved? Our current algorithm does not use confidence scoring or any other sort of qualitative metric; it's just deterministic matching.

5. Business Vault & Curation Layer
Questions:
Business Vault Status: You mentioned this was a weakness in the on-prem implementation. What Business Vault patterns ARE currently implemented in cloud? We have a fledgling Business Vault in the cloud that has 5-10 entities represented, mostly computed satellites and a couple of Business Vault calculated hubs that are different from the Raw Vault hub. We have a naming convention for the Business Vault objects and we have a better approach now, we're a little bit more informed on Business Vault architecture than we were the last time around. Hopefully it'll go better this time.

PIT Table Strategy: Which entities have PIT tables? What's the snapshot frequency (daily, hourly)? No pivot tables yet, but that's definitely in the future. I don't know how far in the future because I haven't seen all the dependencies laid out on a roadmap yet. I've just seen large-scale dependencies so far.
Dimensional Model Inventory: What dimensional models currently exist? (e.g., fact_claims, dim_member, etc.?) Not much of a dimensional model yet. On-prem, we have medical, dental, pharmacy, and vision claims facts. We have a member coverage fact which is designed for counting member months and we have a member eligibility fact which is a time span fact that's designed to track eligibility spans. For members, we also have a risk adjustment fact that grabs the output from a risk calculation process, making it easier to report on and analyze. We have a few other facts based around some claim, side claim information that we get from the government and some banking information that we get from our central finance agency that we use to exchange money with other Blues plans. We have around 120 dimensions that support these facts. I would put our fact count in at around a dozen to 15 or so. Facts, I can't remember all of them off the top of my head. Oh, premium is another one.
ML Dataset Catalog: What ML datasets exist today? Who are the consumers? b our healthcare economics department is for the data science folks, and they do use ML models. They have a process that we refer to as flexible analytics because that's the name of the SAsS provider that we use to calculate those machine learning models and build them. Then we take the output of that data and feed it into our data platform so that they can analyze the output of it.
6. Data Quality & Testing
Missing:
Anomalo Integration: How is Anomalo integrated into the pipeline? When do quality checks run? Anomalo is not fully integrated, but we have a team of domain experts that are responsible for creating data quality checks in Anomalo. That process is just getting started. We've connected Anomalo to our data sources and targets, but I don't know how far their data quality testing has got.
Test Coverage: What % of tables have dbt tests? What's the testing maturity level? All tests on all tables built by dbt have at least rudimentary tests checking for key constraints and not null columns. We also have some data and some actual individual data testing columns for accepted values for date range overlaps and things like that. But mostly technical testing, not business analyst style testing (where you see "does the data even make sense"). We're have mostly tested "does the data operate correctly?" - do the relationships work, are the records unique, and things like that.
Data Quality Rules: What specific business rules are implemented? (You mentioned "consistent recommended tests" - what are these?) We have a package in DBT called DBT project evaluator that looks at some of our testing and makes recommendations. We were also hoping to leverage AI to point out spots in our data that might benefit from additional testing or testing that is superfluous and just a waste of compute time.
Failure Handling: What happens when tests fail in dev vs. prod? Test failures in dev are pretty easy to deal with. They just get a QA fixed branch, we change the code and re-release it to dev, and then we roll back if it's severe enough of a problem. We make sure it works in dev, which dev kind of works as our integration environment, so we're seeing all of our code in one place. Then it moves to Chest Test. If it builds successfully in our dev environment, we sort of call the dev environment at large Team Dev where all the team's code is in develop and that's where it's all running together.
7. Security & Governance
Questions:
Alation Integration: How is Alation used? Is metadata sync automated? Alation metadata sync is active and it's connected to DBT. In theory, we can browse the DBT metadata from Alation. I did that when we first set it up last year, but I haven't been back to see how usable it is right now. We have made a lot of environment changes, so that probably needs to be looked into. It's not failing, but it probably has some inconsistencies since we have changed so many things.
Row Access Policies: What row-level security patterns are in place? Examples? So row-leverage security right now is enforced in a tenant ID with a tenant ID mechanism. Our tables have a column called tenant ID, and right now, it is only Blue Cross of Idaho data. Although we are expecting other tenants to come into our environment. Each company that we do data services for will have a set of tenant IDs, and tenant IDs express the row-level permissions for the table. So for instance in BCI business, we have a tenant ID of 0, which is public information. These are things like code references, like code and description sets, and things like that. They're not really sensitive data at all.Then we have:
- Tenant ID 1 (normal Blue Cross of Idaho membership)
- Tenant ID 2 (restricted membership) for Blue Cross of Idaho employees and executives, and possibly some concierge groups that want special data protection handling. As time goes on, we'll have other company data in there from other subsidiaries of the holding company. At that point, we will create new tenant IDs for them. They'll have a reference table that you can look up the ID values and get all the descriptive information that you need about them.
Masking Policies: What data is masked and for which roles? Data masking at the column level is just data masking policies, and they are primarily to mask PHI and other confidential, restricted information. Things like SPF programs and things that were contractually obligated to our vendors to not divulge to our employees at large but only to a select group of people. Primarily it's for PHI and PII masking for people who shouldn't see those things.
PHI/PII Handling: Specific controls for healthcare data (HIPAA compliance)? As we source new data and bring it into the platform, the process is as follows:
1. The ingestion team does the technical work to get it into a very version of the raw layer that is highly protected.
2. The data governance team goes and looks at that data, running some Snowflake logic to identify columns and categorize them.
3. They take that output and go over it manually, reviewing it.
4. Finally, they approve the table with some of the columns masked if they feel like they're confidential.
Data Classification: How is data classified (public, internal, confidential, restricted)? Yeah, I think this is exactly the terms they use: public, internal, confidential, and restricted. But maybe there's only three. I don't remember all the levels yet. Right now, that's kind of in data governance's wheelhouse. But I can go look. Essentially, if you have access depending on the level of access you more or less data would be masked depending on your level of access.And that is controlled with tags on the object. Those tags are propagated from the raw layer down to any subsequent tables. We have the auto-tag-propagation in Snowflake turned on.
8. Performance & Optimization
Missing:
Warehouse Sizing: What warehouse sizes are used for different workloads? Auto-scaling policies? We have only a very few auto-scaling policies on some of our warehouses. Primarily, we try to use extra-small warehouses for any pipelines that don't need anything more. Small data pipelines and so on. Above that, we have an option in dbt to vary the size of the warehouse depending on the type of load we're doing. If it's an initial build, it uses a medium warehouse; if it's an incremental build, that uses a small warehouse; and if it's a full rebuild, then it uses a large warehouse. We attach that macro to data that is either highly complex and requires a large amount of compute or data that is just really big and requires a large amount of compute.
Clustering Keys: Are clustering keys defined on large tables? Which ones? We have clustering schemes defined on our big transaction tables that are over 50 million or so rows. However, I think we can make some improvements there. I haven't looked closely at the partitioning or the clustering schemes since we set them up, like I don't know a year or so ago. Back then, we knew a lot less about Snowflake than we do now. So it's probably a good time to review those.
Materialization Strategy: When to use table vs. view vs. incremental in dbt? So far, our materialization strategy has been pretty low-key and informal. We do views anywhere that the data flow is really simple (i.e., renaming columns, reordering columns, selecting a small number of columns to forward onto the next step). Those would just be views. If there's any significant business logic, then we typically will materialize those tables. It is a little fluid, so most of our tables in the integration curation layer are physical tables with a few staging views and so on in between them and renaming views. Then our consumption layer is a mixed bag. We prefer views there so that we have less data storage and less latency between the curation and consumption layers. But some of them are going to have to be physical tables because of their size or complex business rules that needed to be tacked on in the consumption layer.
Cost Management: What cost controls are in place? Spending trends? Lots of concern on cost and controlling our spend. The main reason for that is we got an initial budget from the board two years ago, and we have to make that budget last until we have a certain set of milestones that we've hit. We're trying to get that last till midway next year. So there is quite a bit of concern over accidentally burning up a bunch of our remaining credits and not having enough to get us to next summer when our funding gets renewed. For instance, Kafka caused quite a kerfuffle because it started consuming $3-4k/day. When we first turned it on, we made it more efficient, but it was still running at several hundred dollars a day. We finally reduced the batch processing down to 4 hours to keep it under control. I think our next stab at near real-time is going to be a very select subset of tables (maybe 50-100 tables) instead of the 1000+ that we tried to do.
Query Performance: What's the SLA for typical queries? How is this monitored? Query performance is monitored using warehouse timeouts and spending reports. We have those spending reports divided up by tag on different workloads so that we can categorize them. However, that's really the only thing we've done. We haven't looked at a microscope at most of our query performance to see if it's reasonable. Basically, if you stay under about 5 minutes, then you're not gonna get killed by the warehouse timeouts. For the larger warehouses, the time limit is a little longer. I think up to about a half hour or 45 minutes. Processes that take longer than that just get killed, and you have to look at them and tune.
9. Operational Processes
Questions:
Deployment Process: Exact GitLab â†’ dbt Cloud deployment workflow? Our branch and merge strategy is:
1. We have a develop branch that is our main branch.
2. In dbt, we attach to the develop repo, we create a feature branch from that, and then we merge that back to develop.
3. We have a test environment that we promote the code to with a release management process to move from a feature branch to develop as a team lead or a peer review on the code.
4. The release management approval process from dev to test is a team lead code review.
5. The release management process from test into prod is an IT governance like change advisory board council that you go to and get your change approved.
The basic workflow for a developer would be:
1. Go to develop, get the latest version
2. Create a feature branch
3. Make your changes
4. Test them
5. Do a merge request
6. Get a peer review on that merge request
7. Push it into develop
8. Make sure that the jobs are running and that you set up any new jobs that need to be configured
9. Those jobs run at a much slower pace than in prod in dev to make sure that we're not running into new test failures
As we collect enough features that we're ready to release some large group of functionality into test (or even a small group of functionality), they get formed into logical units in our Agile ticketing system called epics. We use user stories counter-intuitively to create feature branches which then get pushed into Dev. All those user stories completed turn into a feature that we then release into test. At some point, we are done testing and we release those changes into prod.

Rollback Procedures: How do you rollback a bad deployment? Deployment rollbacks are handled in GitLab.
Monitoring & Alerting: What monitoring tools? What gets alerted on? Monitoring tools are various right now, but most of them are coming through dbt. Which posts to an internal Teams channel that you can monitor for any jobs that you're interested in.
On-Call/Support: Who supports the platform? Escalation procedures? Right now, the development teams support the on-call and load process. However, we are planning to contract with the Managed Services company to run that for us.
Disaster Recovery: Backup strategy? RTO/RPO targets? Disaster recovery is in the Snowflake domain. We're at a regional backup level, I don't think we're in cross-regional backup, but we do have a disaster recovery plan.
10. Migration & Legacy Systems
Critical Gaps:
WhereScape Details: Which databases/schemas in WhereScape are being migrated? We are not going to migrate any of the WearScape metadata. The on-prem platform that uses WearScape is called Health Data Services (HDS). We are just planning on migrating the capability, not the code. So we do want to run the code. We can use the code as a basis for the new code, but we're not going to write it the same way because much of the on-prem code is not optimized for compute. It's optimized for the load window, which is sometimes overnight or sometimes all weekend. So we have very long running processes on prem.
Migration Phases: What's the phased migration plan? Which business areas first? The business area migration is being incrementally built. We started with provider and member eligibility. We're moving on to expand to the whole membership domain and the whole product domain, and then we'll expand into other domains. As we get a better, more cohesive set of data, then we'll start bringing in business dependencies that we have enough infrastructure to support.
Dual Operations: How long will legacy and cloud run in parallel? That's a great question, still to be negotiated. Our business folks tend to be very reluctant to migrate off of old platforms, but I think we have some executive support this time. So hopefully that'll be fairly painless. Once we get all of the information delivered in the cloud, which is going to take some time.
Cutover Strategy: How do you plan to cut over from on-prem to cloud? So far, the only cutover has happened on some data extracts. What they do is build the data extract in the cloud and then run it in the cloud in parallel to the on-prem extract and compare the output. Once they have a certain number of runs that makes them feel comfortable with the cloud output, then they switch off the on-prem output and use the cloud output to feed to the customer. We're doing something similar with portals. We have a fledgling customer service portal that's being written, and we have a provider portal that we're not going to run out of the EDP but we are going to supply the data to it out of the EDP. So, I think it's going to be a piecemeal migration.
Data Reconciliation: How do you validate cloud = on-prem during migration? The way we validate the cloud, the cloud = the on-prem is just so far it's just been manual data reconciliation efforts, but I think we are going to need to move to a better data reconciliation and validating process because we need to move so quickly. We have some data domain experts in the business that have been identified that can help us with that. But I think a lot of the burden is going to be on IT to solve the problem of how to iterate through testing quickly.
11. Stakeholder & Use Cases
Questions:
Healthcare Economics Use Cases: What specific analytics do they need? What's the MVP? They are mostly concerned with statistical analysis and using the output of their analytics to inform setting rates as they work with our groups to set rates. We do not have a definition of the MVP other than they want all the data. So we obviously can't start with everything, so we'll have to pick something. We are working through prioritization with that group right now.
Provider 360: What data elements comprise Provider 360? Data model scope? Right at 360 is kind of this umbrella term that's very vaguely defined, but it's basically all the information we would need to know about a provider to make good decisions when it comes to our provider relationships and contracting. That's not a very great definition, but it includes a whole bunch of very obvious information that we're still working on pulling. We are working with our provider department to define what things they really do need to measure about the data to help with their current processes.

Portal Requirements: What portals exist or are planned? What data do they need? Right now, we have:
- Member portal and provider portal in the cloud, but outside of the EDP
- Customer service portal being developed, more closely connected with the EDP, and definitely getting its data from the EDP
When we have enough data, the member and provider portals will get their data from the EDP as well. They'll have to switch their data source.
External Feeds: Besides CMS EDGE and BCBS, what other external feeds exist? There is a laundry list of these, and I don't think I remember or have access to all of them or even knowledge of all of them. But it would be the significantly complex use cases that would apply to the consumption layer. If it's just a normal data extract, then we have a data extract framework that our engineers can quickly create an ad hoc data extract and schedule it. Assuming that we already have the data curated and prepped for them, like we've built a process for member eligibility based extracts with which we exchange with a lot of our partners. And so that is like a scripted kind of data set that's already got business rules applied, and all the analyst or engineer has to do is select the columns they want and the filters they want. As we have these more complex consumption layer use cases, though they'll be pretty bounded and they'll probably be fairly complex projects. Not the kind of thing we can contemplate until we have a decent enough integration and curation layer to support them.

12. Technology Stack Details
Missing:
dbt Version: Which dbt version? Any specific dbt packages beyond automate_dv? Dbt version is always set to latest, so as new versions come out, we pick them up. We use a few other dbt packages besides Automate DV - it's just kind of the data vault specific one. We use dbt-utils and dbt-expectations for testing. We use the dbt-project-evaluator package that I mentioned earlier. We use a product package called dbt artifacts which logs test and execution results. We use a package called code_gen that is from dbt labs that we use for generating our staging views. We sometimes use a package called dbt profiler that writes a data profiling set of queries for you. We use another package called dbt synth data that is for creating synthetic data.
GitLab Setup: Self-hosted or GitLab.com? Branch protection rules? Right now, we are GitLab.com, but there's a possibility we'll move back to being self-hosted for privacy concerns and AI use.Our main protected branches are:
1. dev
2. test
3. prod
We may also have a UAT branch at some point that is protected.

AWS Services: Beyond S3, what other AWS services are used? (Lambda, MSK for streaming, etc.?) We currently use both Lambda and MSK, although we are moving away from MSK. We also use Glue, and there is talk of using Apache Airflow for AWS-level job scheduling. And potentially for dbt scheduling if that is a better turns out to be a better method than using the internal dbt scheduler.
MSK/Streaming: You mentioned MSK for real-time - is this implemented or planned? What topics/streams? MSK is implemented for real-time data, but we are backing off of it because of the expense. So that is not going to be part of our future state.
