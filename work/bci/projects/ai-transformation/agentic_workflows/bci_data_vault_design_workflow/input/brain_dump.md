# BCI Data Vault Design Workflow - Brain Dump

**Organization**: Blue Cross of Idaho (BCI)
**Captured**: 2026-01-02
**Source**: Cursor Chat Session

---

## Original Description

### The Problem

What I want to do is take it, create or design a DataVault model, or refactor an existing DataVault model, and create an agentic workflow to implement those changes or the new design. I'd like to use an agent to do pieces of that.

It would be really nice if I could have a simple diagramming tool that I could draw a picture of the hubs, links, and satellites and their connections, and then have the agent start working on the code or the refactoring.

I'd like to make that process as agentic as possible, but **my main goal is to accelerate the current bottleneck that architecture takes longer than the data engineering part of the process**.

I was looking for tutorials that helped me go in that direction or systems that people have already figured out to do that. But I'm not looking for new commercial tools to help with that. I have a sort of a set tech stack that I'm working with.

### Tech Stack (BCI Environment)

- **Snowflake** hosted on AWS
- **dbt** for data transformations (most of the code we write is in dbt)
- **dbt Copilot** - available for engineer acceleration
- **Microsoft Copilot Chat** - available
- **Claude via Amazon QuickStart** - limited version of a Claude coding assistant

Working within those constraints, I'd like to find some tutorials that will help me start making our workflows more AI-native.

### Two-Environment Constraint

**Critical**: The workflow will be designed in one environment but used in another.

| Environment | Purpose | Tools Available |
|-------------|---------|-----------------|
| **Design Time** (Cursor + Claude) | Build, test, iterate on workflow | Full Claude capabilities, Cursor automation |
| **Runtime** (BCI VSCode) | Where architects/engineers use the workflow | Amazon Q extension, Snowflake Cortex AI, MS Copilot Chat, dbt Copilot |

**Implications**:
1. Agents = portable prompts + instructions (not Cursor-specific automation)
2. Specs must be self-contained (no dependency on Cursor context)
3. Prompts need to work with Amazon Q, Cortex AI, and the available tools
4. Design and improve the workflow here in Cursor; deploy artifacts to BCI VSCode environment

### Runtime Tool Roles (Clarified)

| Tool | Role | Status |
|------|------|--------|
| **Amazon Q (VSCode)** | Primary workhorse - spec generation, prompt creation, code evaluation | ✅ Core |
| **dbt Copilot** | Actual dbt model creation (receives prompts generated by Amazon Q) | ✅ Core |
| **MS Copilot Chat** | Chatbot only, awkward as coding assistant | ⚠️ Awareness only |
| **Snowflake Cortex AI** | Not yet implemented at BCI | ⚠️ Future option |

**Workflow execution**: Amazon Q does the heavy lifting, generates prompts for dbt Copilot, and handles evaluation. The other tools are noted for potential future exploration but are NOT dependencies.

### Context Sharing Between Environments

- ✅ Can bring general context and tooling info into Cursor for design work
- ❌ Cannot bring proprietary or protected BCI data
- ✅ Can use Cursor for design process with sanitized/generalized examples
- ✅ Templates, patterns, and prompts developed here can be used in the BCI environment

### Asymmetric Data Flow Constraint

**Critical workflow constraint**: Data flows easily IN but not OUT of the BCI network.

| Direction | Difficulty | Method | Frequency |
|-----------|------------|--------|-----------|
| **Cursor → BCI** | Easy | Copy files to private network | Anytime |
| **BCI → Cursor** | Hard | Email with security checks | Weekly or less |

**Limitations on export (BCI → Cursor)**:
- Requires email
- Goes through security checks
- File size and attachment count limits
- Manual, time-consuming process
- Must avoid sensitive/protected information

**Solution: Context Sync File**

Create a standardized "context sync file" that:
1. Is designed to be safe for export (no PHI, no security-sensitive details)
2. Contains project status, progress, and non-sensitive metadata
3. Can be generated/updated by Amazon Q inside BCI
4. Is exported weekly (or as needed) via email
5. Enables continuity between Cursor design work and BCI implementation

**Safe to include in context sync**:
- ✅ Code style patterns (generalized)
- ✅ Status updates (what's complete, pending, evaluated)
- ✅ Workflow progress tracking
- ✅ Template structures
- ✅ Lessons learned (generalized)
- ✅ Prompt effectiveness notes

**NOT safe to include**:
- ❌ Protected health information (PHI)
- ❌ Security-sensitive infrastructure details
- ❌ Specific internal system configurations
- ❌ Proprietary business data
- ❌ Anything that could pose security risk if exposed

### Implementation Guidance Needed

**Key constraint**: User is accustomed to richer environments (Cursor) but must implement in a more constrained setup (VSCode + Amazon Q). Workflow deliverables should include:

- Guidance on Amazon Q capabilities and limitations in VSCode
- Troubleshooting tips for common issues
- Clear instructions for how to use prompts in that environment
- Notes on what's possible vs. what requires workarounds

**Current setup**: Using Claude Opus 4.5 inside Amazon Q - works well, but unfamiliar with full VSCode/Amazon Q capabilities.

### Diagramming

Current diagramming tool is **Lucidchart**. I don't know how well that would work in a workflow.

### Vision for the Workflow

The goal here is to take our design decisions and turn them into specs that a data engineer can work from. **The more AI-ready those specs are, the better.**

#### What I Want to Support

1. **Architecture Process** - I'm already comfortable making architectural decisions and discussing those with other architects
2. **Specification Writing** - Need help writing the specification with enough information for a data engineer or coding assistant to take that refactoring/design and implement it in code
3. **Code Generation (Future)** - Eventually would like AI code generation, but not the first step
4. **Code Evaluation** - Want an AI evaluator for the code based on acceptance criteria from the specs
5. **Human Review Support** - AI process to test code against specification and identify things for a human to review in more detail

#### What I Don't Want (Yet)

- Don't want to take the data engineer out of the loop right now
- Think I'll need to retrain data engineers to evaluate code more than write code, but we're not quite there yet

#### Key Outputs Desired

- Specs and evaluations that work equally well for AI-generated code as for human-generated code
- Starter prompts for dbt Copilot as part of the output
- If possible, work from diagrams; if not, text descriptions are acceptable

---

## Parsed Understanding

### Current State
- Architect makes Data Vault design decisions (hubs, links, satellites)
- Discusses designs with other architects
- Currently using Lucidchart for diagramming
- Once design is complete, data engineers implement in dbt (Snowflake/AWS)
- **Bottleneck**: Architecture → Spec handoff takes longer than the actual coding

### Desired Future State

1. **Captures architectural decisions** (ideally from diagrams, but text is acceptable)
2. **Produces detailed specifications** that are:
   - Complete enough for a data engineer to implement
   - AI-ready (could work with dbt Copilot or future AI code gen)
3. **Generates starter prompts** for dbt Copilot to accelerate engineers
4. **Evaluates code against specs** - AI-powered review that flags items for human attention

---

## Clarifying Questions - ANSWERED

### 1. Current Specification Format
**Answer**: Has a structured template for handing specifications to engineers.
- Template exists and is in use
- (Template details to be added as context)

### 2. Design Input Granularity
**Answer**: Single entity per work item.
- One hub, link, or satellite to modify or create
- This is the unit of work for an engineer
- Note: May need to evaluate if this is optimal for AI, but matches current engineer workflow

### 3. Source Information
**Answer**: Various formats, can bring sanitized versions to Cursor.
- Source documentation exists in multiple formats
- Can be brought into this Cursor environment for prompt/template refinement
- Will add as context over time

### 4. Current Review/Validation
**Answer**: Human review via GitLab merge requests.
- Private GitLab instance
- Merge request submitted from dbt
- Architect or lead engineer reviews and approves
- Approval gates to develop branch

### 5. Lucidchart Usage
**Answer**: Currently fluid/freeform, willing to standardize.
- No strict conventions currently
- Open to establishing naming conventions
- Open to adding metadata to describe diagrams
- Opportunity to make diagrams more AI-parseable

---

## Phase 1 Complete ✓

All clarifying questions answered. Proceeding to:
1. ~~**Phase 2**: Real-World Process Specification~~ ✓ Complete
2. **Phase 3**: Agent/Human Allocation ← NEXT
3. **Phase 4**: Agentic Workflow Specification

Reference prompt: `@agentflow` (ai-resources/prompts/workflows/agentflow-workflow-designer.md)
