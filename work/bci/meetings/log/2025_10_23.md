# Data Source Discovery with Abacus. 
Went through some final questions about the data source discovery with Abacus. There are still some questions left, but they're going to require focused discussions with some subject matter experts from the business as well as IT for specific sets of use cases.

But other than that, we've gone through all the data sources and defined them, talked about the needs and the timing for them, and what format to ingest them in. And so I think they have all that information. Now we just have a few breakout sessions to do over the next week or so to define some of the details around individual data sources and the specific use cases that we need to support for those. 
Lorraine offered to send the transcript for the meeting to me, so I will ping her tomorrow if I haven't seen those. Then I can use the meeting transcript to turn into a summary in my journal, which will be more accurate than just me trying to recall what happened. 

# DBT Overview 
Gave an overview of DBT to Tamra Hess, Dave Johnstone, Sam Jesse, Leonardo Rizzulli, Troy Brunson, and Scott Watson. Lakshmi Chandran was also there. She's the one who organized the meetings. We went through and made sure they all had developer access again. They haven't used dbt for like a year. And they're going to start using it again for some pipelines. And so they had some training at one point, this is a refresher course. So I made sure they had the right access. I showed them the interface and how it explained how it interfaces with GitLab and with Snowflake. Informed them that they could develop dbt code in VS Code if they want to. Although we don't have too many people doing that right now, it's theoretically possible. We went through how to run jobs, how to organize those, and what they look like. We went through the model repository, part of the code repository, and how those things are configured and sort of the basics of how dbt expresses how you write code in dbt and how you write your tests and configuration stuff. 
Next session, we will work through a specific example with the team. Then, they can take that example and apply it to some of the work they already have on their plate. We might have another session to review the code that they wrote. 

# Snowflake for provider meeting 
Met and talked with Joe Sheer from Provider and Lindsey Smith. We tried to walk Joe through the suggested metrics, and he gave an example of a metric which I thought was really good:

"Hey, if we want to send provider information off to report on network adequacy for providers, can we get a list of all the providers for this month, the networks they are part of, and their primary location? Then we can compare their specialties and the specialty coverage in each zip code to see if we have adequate network coverage. I think is how it works." And so I think that's a question that we can answer out of the provider fact. I just need to think about how to work that into the data model. But it seems really useful. So that is just one example. But I thought it was a good one that we hadn't captured before. We also walked Joe through the Snowflake interface and what that would be like for his analysts to work in and write queries against. He seemed very relieved that they would have that access.

Then we talked about the use cases that his team currently satisfies out of the sandbox database, which we don't want to replicate in Snowflake but we are planning on replicating capabilities of the sandbox database in HDS. So those capabilities are:
1. The ability to load data, side-load data temporarily, for use with other queries
  - Side loading data in Snowflake will be accomplished by using the manual upload into a temporary table. Those will get deleted, not temporary as in temp table but temporary as in a physical table that just doesn't stay around for very long. The DBAs are going to create a clean-up process that will go behind and delete anything that is missing in a couple of weeks. This won't be a permanent storage, but they can keep it there for a week or two if they need to. 
2. Reference data
  - For reference data, we're going to load that into the raw vault and then make it available wherever they need it in any of the downstream layers. It will be a little more work, but if it's permanent reference data that they need, we can do that and that gives them access to it. An example might be a provider taxonomy and description of taxonomy codes. Information that they seem to use quite a bit. That's just industry standard information, we'll find a source for it and make it available to them. 
3. Persistent data that they need to keep around that is the result of their analysis
  - Persisting data is a little trickier, and we are still talking through the architecture of how we can deliver that capability without sacrificing any security. Since if we give them the ability to create their own objects, they could in theory create the objects without data masking tags and so on being involved, and then that would open up the data for other people to use it who may not be allowed to see those data elements. 

# PCP Attribution Sync with Martin
Martin showed me his work so far on PCP attribution. Not super impressive, but this is his last week, so I didn't push the issue. There is a spreadsheet that he's creating with his take on what things in the business rules make sense and which things don't. I think that will be valuable for me to compare my interpretation of the rules against. 

# EDP Clinical Model Review 
