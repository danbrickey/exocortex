# EDP terminology alignment. 
We had a meeting with the data governance team to align on terminology. We discussed Some of the documents that I produced using my refactoring process suggested a couple of tweaks:
1. That we should call them something besides business rules, so we're going to go with the term "logic guide". I showed them my tweaked prompt for that, and they seemed excited about the results.
2. They're going to start by meeting with the provider folks and talking through the PCP attribution rules, and see what their feedback is on that.

# Dbt error work party. 

Met with Sumedh, Carl, and a few of the data engineers to work through some of the dbt errors that are occurring in the test environment. We made some progress. We went through the logic for the date range calculations. This is a temporal range merge operation that consecutive coverage is doing. The goal is to have all of the gemstone membership reach back into the legacy eligibility for members who were converted to gemstone and represent their continuous coverage under their gemstone member ID. Then we want to add to that any gemstone members that were not converted but were created in gemstone and any legacy members that were not converted, that were terminated prior to their eligibility term prior to the gemstone conversion. If we can get that to work, then we will have no overlapping eligibility issues. But that's proving a little tricky, and I think that the date logic that we got when we did the refactoring using Amazon Q is not quite right. So we are starting to tweak that. In fairness to the AI output, I don't think this code was working the way it should have been prior to the refactor. 

# Amazon Q access experiment. 
Spent some of the day experimenting with my Amazon Q access and quickly determined that this has some pretty limited context and usage rates for the free account. So I'm going to contact the admins on the AWS side and see what I need to do to get an Amazon Q Pro account and see if that works any better. When I ran my simple refactoring problem, which is refactoring 3NF tables to raw vault tables, that went pretty well. But it is a fairly limited set of information that we need for that.

The other refactoring process I tried was the refactoring of HDS business vault/dimensional model architect artifacts porting them over to EDP. That's a significantly more complicated process with lots and lots of input and what I discovered because we feed it the old code for that entity and a lot of context and guiding documents. I tried the smallest one that I could find and even that one blew the context limit right away in so decisively that Amazon Q wouldn't even process that request. It just said too much context. We might be able to chunk the context up somehow to avoid using the context window. But I'm afraid that a lot of it is that legacy code. Thousands of lines of code. And I think that's just too big of a chunk to do it. But there's not necessarily a smaller logical chunk without doing a lot of work. So I'm hoping the Amazon Q Pro account will help with that. 
