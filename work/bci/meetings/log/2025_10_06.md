## Member Domain Use Case Meeting. 
Met with Martin Rivera from Hakoda to discuss use cases inside the member domain that we can start with as we start migrating code from our on-prem data warehouse to the cloud. We discussed possibly using Warescape to try to harvest the code and metadata from the on-prem EDW. We requested access to both the On-Prem EDW and Warescape. Warescape licenses are quite pricey, so we'll see how that request comes back. 

## Abacus EDP roadmap meeting
Met with Abacus and Lindsay to go over the EDP roadmap for the next year and a half. Abacus has some good questions, but didn't really feel like they connected with the larger EDP roadmap and how it fits with their role in providing data for the EDP. They do seem very competent about their interop agreement. There are two main use_cases:
1. A FHIR-formatted feed that goes out to CMS to fulfill some regulatory requirements
2. And providing raw and somewhat integrated data to the EDP raw layer so that we have less business rule processing to do on the data to get it where we need. And less of our hours go to tedious building tedious ingestion pipelines, and we can focus on the more sophisticated business logic. 
I think the meeting went well, although I think they didn't quite grasp the whole project. But I think they understand their part in it. 

## Scrum of scrums meeting for the EDP. 
Reviewed the in-progress major work items for each team and discussed the architecture implications and whether we were on schedule.  We also discussed what things might be useful in a collaboration sense to cover in that meeting. We talked about possible collaboration points between teams on the EDP as well as shared services teams outside of the EDP. 

## BCI Architecture Alignment 
Sani and I met with Hakoda, Emily, and Odell to go over their ingestion workflow that they're envisioning for both the analytic workloads and the real-time workloads. This was in regards to how we get the data from the S3 bucket into Snowflake. There are a couple of different patterns at work there:
1. For files that come in daily or less frequently. Don't need quite as much handling as files that have a high volume of data coming in all the time.
2. High-volume transactional data needs a little bit of special handling as it comes into the EDP. 
For less frequent file feeds, the files come in and they just get picked up sequentially and processed because we don't need to worry about data coming in on the heels of that data if it's just daily or weekly or something. 
For the high-volume transaction type data (like ADT information), where we get it in small batches continuously throughout the day, they built a queuing system so that the files get put into a queue and then a process reads from the queue and processes the file and reads from the queue, grabs the next thing from the queue. This gives us the ability to be multi-threaded if we need to and also to not miss a file load because we were in the middle of loading something else. 

## Eligibility Flattened Table Testing. 
Met with Carl Morse and discussed the Eligibility Flatten table, which is the source table for all of our eligibility extracts. There is some kind of issue with the date logic. We are coming out the other end with some overlapping date ranges. I suspect there are some dates going in that don't really make sense and the process is trying to make sense of them. Carl helped me identify some specific cases where this is happening. I have about ten, and then I am going to figure out who I can delegate this to to troubleshoot. I think this one Jason or Shreya could handle really well. 

## Near Real-Time Design Review. 
I met with Ranga from Snowflake and went over his diagram of how the near real-time data will flow and how it will be separated from the analytical workloads and what points it will share information with the analytical workloads. I think we have a good shared understanding of what we're going for now. He is going to think about any architectural implications that might trip us up and come back with any suggestions that he has for it. But it sounded like we were kind of on the same page with the approach. 