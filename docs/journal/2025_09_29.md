## Dev Raw layer meeting. 
Discussed how to keep the use of the raw layer for the ingestion development team and the use of the raw layer for the data engineering team from clashing with each other. Decision was to create a clone that we would use for the data engineering teams, separate from the clone that we use for the ingestion development. So the dev environment would actually have two raw layers. 1 is the normal dev_raw_db this one would be used by the data ingestion teams. The other would be: dev_raw_clone_db this one would be used by the data engineering teams. Ian is setting up that clone for the data engineering teams today, so we should be able to switch our references to point at that tomorrow. 

## EDP roadmap meeting. 
Met with Dave Algren, Cassie Morton, Rhett Barton, and Lindsey. We discussed the prioritization and commenting on some of the data source requirements that the HCE teams had.  Lindsey is starting to work those into the EDP roadmap and trying to find places for them that make sense with the dependencies that they have. We will probably need another session or two to comb through all of their comments and make sure we understand them. The meeting went well. We talked about the types of use cases they have. The three they identified were:
1. The ability to load reference data that they need to supplement their analytics
2. They also need the ability to feed the output of their analytics processes (like machine learning models) back into the data and persist those results. 
3. Then they need also the normal analytical capability that comes with our dimensional models and so on. 

These three use cases might be solved a different way in the cloud than they were on-prem. The on-prem solution for the first two is to give them a sandbox database that they can do whatever they want. And then the third use case is served by the EDW 2D model and some of the other infrastructure in the health data services platform. But that's on-prem.
 
In the cloud in Snowflake, we don't have the latitude from a security point of view because it's such a highly integrated and highly leveraged platform in terms of how many users and use cases are being served out of it. We may need to build them an application, a Streamlit tool that allows them to upload reference data. And then give them the ability to create, to persist some of their output into the raw layer through some kind of an approval process where they create the data or they record the output and then they request a table in the raw layer to incrementally load from that location in the consumption layer. So we have a feedback loop for our data product. And then the analytical capabilities are already use cases that we were planning on replacing in the cloud equivalents to the on-prem capabilities. In theory, that for instance, would cover all those three use cases. They may have other use cases, and those spitballed solutions may not be the ones we end up with. But I think it's clear that we will need to solve for those use cases in a different way in the cloud. 

