
## 8:30 am management review and problem-solving meeting. 
We discussed the decision of whether to rename Franken or clone Franken before renaming when we are setting up the new dev environment. That decision hasn't been finalized and probably will be finalized today or tomorrow. 
We also talked about the incoming data from the association and one of the files, specifically the MMI file that is not currently in production. That's okay because we don't need it for quite some time, I think. 
We also discussed the switch from Kafka as we start to retire some of the Kafka loads due to expense. Do we keep the Kafka in place for the ones that are real-time, or do we change the ones that have near real-time needs to CSV and do the necessary changes to the S3 bucket so that the CSV ingestion can be very quick. 
We discussed the interoperability partnership with Abacus and what that might mean for the assignment of tenant ID in our enterprise data platform. Whether we would have Abacus apply it for us or whether we would do that ourselves on our side. That's probably too long of a topic to discuss in this meeting, so they're going to schedule a separate meeting for that topic alone. 

## OneView unstructured data innovation meeting. 
We discussed a few different AI opportunities with the One View application to leverage AI:
1. Harvest information from contract documents that are PDF images of the signed contracts with our groups so that customer service can see the most recent group contract if needed during a call.
2. Another idea was summarizing text and customer service notes so that customer service agents can see a summary of all the notes that have been taken on this particular customer. And also to read through the text descriptions of benefit information for and extract a more structured piece of information. 
We decided to have a series of meetings that this proof of concept would need to happen in Q1 of 2026. Before then, we want to discuss some detailed design documents to make sure we have all the raw ingredients ready by the end of this quarter for doing a proof of concept. 0We decided that option number one would be the most beneficial, which is to read in contract documents that are in a document store and output some summarized information about those documents that can be reviewed by a customer service agent, and the document can be pulled up. So we're going to continue brainstorming, but that seems like the likely use case we're going. We are going to have a series of meetings over the next couple of months (not too frequently) to get all of our design decisions made and a design document put together to guide the work for the POC next quarter. I volunteered to organize those meetings and track our progress in terms of coming up with a design document. 

## Discussion on ingest presentation. 
I spoke with Sonny and Rom about the upcoming presentation in the ARB next week. This presentation is for ingestion methods, whether we're going to mix MSK and S3 CSV files and how quickly we're going to move away from Parquet files to only S3 CSV. The recommendation is to move to CSV files in S3 for everything. Sony felt that the CSV performance would be good enough with partitioning and rolling files into Glacier storage that the performance would be fast enough for the near real-time use cases we have. That would certainly simplify things, but the MSK (partial MSK) is still a possibility. 

## Snowflake environment hardening meeting. 
Spoke with Emily from data governance and Lindsay about the process for hardening the Snowflake environments. One of the key decisions that hasn't quite been made is whether we rename or clone the current Franklin environment into a new dev environment. And I think we're leaning toward renaming Franklin over to dev. 

## EDP business alignment stretch goals. 
I met with Lindsay, Data Governance, and Emily, I think. We were discussing a stretch goal for this PI, which is to try to align our EDP prioritization and roadmap with the business. To do that, we are going to form some councils. They would involve leadership and data domain experts from our different business domains and contexts. For instance, membership, provider, claims, broker, and those types of divisions. We've already formed a Provider Data Council that we have been meeting with, and we want to start the same process for membership and product. In parallel, Hakoda, our business partner, is planning to begin work on an MDM solution proposal. Hopefully, next quarter, we'll go hand-in-hand with the work that comes out of this business alignment and these business domain councils. 

## Franken dbt transition meeting. 
We mostly spent this meeting talking amongst the data engineers that attended about some questions that we had for the upcoming transition as we try to move our development environment into the North Star account. We discussed timing and tentatively decided to start tomorrow morning at 11. We will schedule a 3-hour working session, which will be an open house style. People can drop in and drop out as they need to for other meetings. The goal of the session will be to switch our development environment to point to the current Franken environment in Northstar. This is the environment that will eventually be renamed to the dev environment in North Star. One thing that could postpone this work is if the DBAs or the Snowflake admins decide to rename that environment before we have an opportunity to make the switch, if they want it to be already renamed to dev before we bring all the data engineers into it. However, the data engineering teams don't wanna wait too long because they don't want the work to stretch on into the quarter. 

## AI proof of concept demo. Migrating business use cases from on-prem to the cloud. 
We presented to David Yu the results of our AI proof of concept where we were trying to migrate a whole domain of data from on-prem into the cloud. This effort represented about 200,000 lines of code from on-prem. And the output was supposed to be new dbt code models and so on for our architecture in the cloud. The result was somewhat plausible code, but not very usable code. Lessons learned:
1. We probably bit off a little more than we can chew
2. We need to reduce the scope of the effort
3. Focus on a set of related business entities within a domain, such as provider registration, certification agreements, and relationships and affiliations
4. Work on a portion of those at a given time to avoid overloading the context window of the AI

Ram and David seemed very receptive to the lessons learned information and were pretty positive about moving forward with it in a sort of hybrid approach between the two methods we were considering. One was lift and shift One data domain at a time into the cloud. The other one was to use AI to refactor the code and put it in the cloud. The proposed recommended approach turns out to be a hybrid between those two. Take one data domain at a time and move it, refactor it with AI into the cloud. 