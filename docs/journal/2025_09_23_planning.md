## 8:30 am management review and problem-solving. 
Discussed the finalized plan to rename Franken to Dev in Northstar and to begin the transition of the dbt development environment to Northstar immediately. 

## PI 3.2 planning wrap-up meeting. 
We went through all teams' planning from the last week, analyzing the dependencies and the cross-dependencies, as well as the sequencing of events. We talked through some of the impacts of the architectural decisions made this week. One of the outstanding sentiments of things that we can do better in the future is to make some architecture decisions beforehand, before the PI planning week starts. The difficulty here is that we frequently need to talk to teams multiple times to make those architecture and timing decisions which eats into those teams' time at the end of the previous PI, which is usually sensitive. 

## DBT transition to Franken
We started the transition to Franken. We made the necessary changes in the EDP Source Data Project, and those things seemed to go well. The next step is getting our branch merged in, which we can't currently do because there's a policy scan issue. I reached out to Sean Trent to see if he could help us. I haven't heard back from him. I'll try to expand the request to the rest of his team. Once we get that code merged, we'll be able to test the develop branch jobs. And then we'll move on to test and prod. Once those are working, we'll move on to the EDP data domains project and go through the same steps there. Once all that is finished, all of our development would be on North Star environment, and we'll be ready to start talking about deprecating Rising Sun environment. Immediately after this, we'll probably start working with Jason and Shreya and the OneView team to separate out their pipelines and create a separate repository for them. We'll start refactoring those pipelines to work more closely to real-time. But we're planning to be done with the transition to Franken by mid-week next week. 

## Snowflake office hours. 
The proposal during Snuff Lake Office Hours was to alter office hour format a little bit and have people bring to the meeting problems that they are encountering as part of the current PI so that we can work on immediate problems. The problem we chose to talk about this morning was tracking AI resource compute spend, and we had an alarming increase last week in the AI spend in Rising Sun, so we wanted to track down the cause of that. With Ranga's help, we were able to figure out what we were missing from our investigation and queries. Turns out a lot of the Cortex AI usage history is actually in different views than the rest of the usage history. We also concluded that we probably should have Ranga find an AI expert at Snowflake that we can have some time with to discuss realistic and affordable Cortex proofs of concept instead of the ones that we've been doing. Some of the results of some of them seem more expensive than what we want to pay for for that type of solution. 

## Review near real-time design options. 
In this meeting, we discussed the logistics and architecture behind switching some of our pipelines to be near real-time. There are three different options that were put on the table:
1. The way we are currently doing it. We have a working solution that hasn't been implemented because it was created for the Kafka pipelines, which we are moving away from. However, we do have a way of ingesting our normal data (on the normal batch cycle of 4-6 hours) and putting a view on top of it that will read the information that is waiting to be ingested from the stream and creating a UNION ALL view of it. This way, we actually have a view if you want to pay for the compute to see the real-time data pretty close within several seconds of the source system.After this view that we're calling a deferred merge view because it's a view that allows us to defer the merge until later. We would have a layer of dynamic tables in the curation layer, possibly in the integration layer, to apply the business rules for integrating and cleaning up the data on the way to the OneView app. 

3. Option two is to use triggered tasks for ingestion rather than our streams. The triggered task would read from the streams, but it would enable us to refresh every 15 seconds rather than every minute like a dynamic table limits us to.

We would consume from these triggered tasks either a view so that it was dynamic or a dynamic table if we need to for performance, and that would be in the raw layer.

In the curation layer, we would have another layer or two of dynamic tables to apply all the business rules. So pretty similar downstream to the solution one and it's also similar to solution three. Really the big variance between the options are the way we ingest the data. 

3. Option three would instead of using triggered tasks, we use a dynamic table in the raw layer that would refresh every minute. Although the current minimum for dynamic tables (1 minute) is going to get pushed lower by a future release of Snowflake down to 15 seconds, so we still may be able to turn up the speed there. So, here we would have a dynamic table in the raw layer, another set of dynamic tables in the curation layer, and everything else will be views. Latency is pretty low. 

All three options have low latency, but the first option has the advantage of already having a working pattern in place that we developed for another purpose that we can reuse. Hopefully, the work to exit investigate option one would be a lot less than the other two options. So I think we're going to start there. 
