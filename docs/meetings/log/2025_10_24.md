# AI governance council.
Attend this meeting to get our request approved to use ChatGPT models in Snowflake Cortex and to use Snowflake Cortex for a proof of concept on analyzing the content of PDF documents containing contract information to summarize benefits for a customer service agent.The request was approved, so we are in the clear to do our POC and set up Snowflake Cortex in our sandbox Snowflake account, risingsun

# Tenant ID Table Structure Review
I met with Tina to discuss the architecture that we're going to use to implement the row access policies in our future architecture. There's going to be a tenant ID column, or there already is. We're going to add one to raw. Data Governance is going to flag all those columns and assign a tenant ID value, and then the row access policies will be based on the tenant IDs you have access to rather than a complicated lookup table that takes a lot of compute every time we run a query.

# Boomi introduction demonstration.
This was a meeting with representatives from the company Boomi, which makes an API management environment that is AI-assisted.So, it looks like a great tool for APIs. It does not look like a great tool for data transformations. Some of my questions were around that, and it sounds like we want to do all data transformation, anything complex, outside of Boomy before Boomy gets a hold of the data, which makes sense. It's not an API's job to actually process data.

# New EDP streaming repo
I created a new repository in our EDP GitHub organization for streaming data pipelines. This is separate from our batch data pipelines repo. The idea is to keep streaming and batch separate because they have different architectures, different requirements, and different deployment processes. This will help us manage the code better and avoid confusion.
I met with Jason and Shreya to discuss the redesign of the repo and moving the streaming code into this new repo because we had planned on doing that prior to them starting on the streaming the new code for the streaming pipelines.
They had already started because of some time pressure on delivering the thing. So we're going to switch the order around a little bit. They're going to write the new streaming packages or pipelines in the existing data domains repository. And then we will copy their repo and create a separate version of it when they have a sprint that they can do dedicate to that. Right now they have some deliveries to do on the project.We spent this meeting discussing where they're at on the project and what support they might need as they go through this refactoring process.Sounds like things are going well. They plan on being done by November 4th, and then we can start working on the repository split.