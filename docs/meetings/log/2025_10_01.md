# EDP roadmap meeting. 
Lindsay and I Met with Dave Algren, Rhett Barton, and Joe Shear for the follow-up to the meeting on Monday. We discussed the prioritization and description of the HCE department's analytics requirements. We got details on some of the more specific use cases that they listed out as a department. One of the decisions we made was that they were going to start tracking each table while they went through their daily, weekly, and monthly business processes. They are going to log each table that they have dependencies on for that process, and then they will give us that list after a month or so of filling it out. I think we got clarification on some of the things we needed and details on the things we were unclear on in terms of priority. I think we need to refine the road map based on dependencies as well as priorities. What we have now is mostly a prioritized list, and what we need to do is track a few of the major dependencies just to make sure that we will have everything in place to actually do the work where the road map says we should. 

# Abacus Hakoda & EDP architectures. 
Sani and I had a conversation around questions I had about how our interoperability agreement with Abacus is actually going to work as we merge our architecture with their data ingestion capabilities. I was just clarifying with him that the Abacus bronze and silver layers are going to flow into our raw layer, or if we are to source our curation layer from Abacus's silver layer. The decision or the clarification was that we wanted to retain control over how the data was integrated because we are going to have data that we ingest ourselves, and we are going to need to have exert control over some of the security measures like tenant ID. In addition, from an architecture point of view, we really do need the foundation of the raw vault that we can extend into the business vault to stick with the methodology we have chosen. So reading data directly from the Silver layer that Abacus provides to us would mean that we would need to adopt some other type of architecture. This way, we keep all of our raw vault methodology issues that we solved for in our solution using the Data Vault 2.0 methodology. We don't have to rework any of those. We are going to continue down the same path, Abacus Bronze and Silver data will flow into our raw layer. And from there, we will process them with data pipelines as we currently do. The reason we're sticking with this even though it's a little bit more work is that we would now have an opportunity to generate this code through AI processes and to refactor our on-prem business rules into dbt pipelines using AI processes. We are really hoping that that part of the process becomes less time-consuming, and that the additional infrastructure that we're going to build to make this happen will primarily happen in a very generated code generator kind of way, with just a human reviewing the results. Anyway, we're hoping the cost of the extra work goes way, way down over the next few months. 

# Provider Data Product Council. 
Met with data governance, Lindsay, and some of the folks from provider to discuss the provider engagement data model. They needed a few clarifications. Lessons learned here as we communicate with these business data councils: we need to be very clear about the straw man or pilot model that we put up for them to start the discussion. We also need a better way of diagramming, which we talked about with data governance a few days ago. So that we can align with business terminology for higher levels of our diagram, but still be able to drive down to the detail we need to actually do the engineering work to build it. The topic today was the term "Provider Engagement" fact, and that was really a straw man term that we just tacked on at the beginning of the proposal to say, "Hey, we could build something like this." We use that term, but it turns out that that term is already in use in our business, and one of the teams is a provider engagement team. That means something very specific to that department. We're going to change the name. We had some suggestions kicked around, and the other thing we had to clarify was that the proposed metrics were also just a straw man - sort of examples for us to pick apart and decide what we wanted to keep and what we want to throw away. So I think we're clear on that. They're going through the metrics now to see if they really do what any or to get inspiration from them to come up with their own metrics that measure pain points they want to ease. 

# One view benefit summary from contract documentation and proof of concept meeting. 
We discussed what steps we need to take to prep for a proof of concept next quarter, which is to ingest some contract documentation into Snowflake, either with an external or an internal stage. This documentation is in images and PDFs. We want to take this unstructured data, use Snowflake, Cortex-AI, probably the Arctic model, to extract information from these documents. Summarize the information in the document for a customer service agent. Or provide keywords from the document so that we can search and find that document during a customer service call. But some of the contracts are quite involved, so we will need an easy-to-understand summary of the benefits provided or specified in the contract for a customer service agent. One of the next steps is that we need to get from the customer service department a list of frequently asked questions - questions that they need to answer while they are on the call with a customer, especially around these contract details: the benefit contract. This contract is typically a group-level contract, but some of them work a little differently. But the customer service agent can't read through pages and pages of legal documents to help the member understand their benefits. We'll need to summarize them.

We need to know that list of some of the questions that they really need answers to, so that we can try to get those specific things from the contract language. So Jesse volunteered to help collect that list of questions from the business, he's going to work on that. The next step for me, I think, is to take some of the contract documents and work with data governance to make sure they don't have PHI so that we have some documents to work with. We can set up a really simple test of the whole infrastructure and make sure we can get the documents into Snowflake with some kind of a stage, and then make sure that we can see those documents, we have the permissions to run Snowflake Cortex functions and so on. 
Part of the output of this PoC besides proving that this concept could work for our customer service application is the broader set of use cases around recreating structured data from unstructured data that we have a lot of. We can do a similar thing with claims images and so on. Then maybe that would increase our ability to leverage this type of pattern in other places in the business. As well as being a proof of concept for one view, which still makes it valuable, we should come out of this with a development pattern for handling unstructured data in our analytics that our company can use. 

I need to schedule a meeting with the same group in two weeks to review our progress on those first steps. 

# DBT testing. 
Worked with Jason and Shreya to try to troubleshoot the problem that we're still having on the new dev environment. The name 'franken' is still in the mix somehow, even though we've checked our project connections and our development credentials, and all of our database name expressions that are based on the environment variables seem to be working. Somehow, there's still a call to the 'franken' environment in there. We opened a ticket with DBT support, and they asked for some follow-up information that I will send them tomorrow morning. So there's still one little technical wrinkle before we have full development capabilities, but we're making progress on it.  


